{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__ (self,structure, tweet_data, source):\n",
    "        self.structure = structure\n",
    "        self.tweet_data = tweet_data\n",
    "        self.source = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile train data\n",
    "\n",
    "path = './Datasets/semeval2017-task8-dataset/rumoureval-data'\n",
    "\n",
    "# read data\n",
    "\n",
    "lis = []\n",
    "folders = os.listdir(path)\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    fold = os.listdir(path+'/'+folder)\n",
    "    for i in fold:\n",
    "        if (i == '.DS_Store'):\n",
    "            continue\n",
    "        temp_source = path + '/' + folder + '/' + i + '/source-tweet/'\n",
    "        temp_replies = path + '/' + folder + '/' + i + '/replies/'\n",
    "        temp_struct = path + '/' + folder + '/' + i \n",
    "        with open(temp_struct + '/structure.json') as f:\n",
    "            structure = json.load(f)\n",
    "        source_file = os.listdir(temp_source)\n",
    "        source = source_file[0].split('.')[0]\n",
    "        tweet_data = {}\t\n",
    "        with open(temp_source + source_file[0]) as f:\n",
    "            tweet_data[source] = (json.load(f))\n",
    "\n",
    "        reply_file = os.listdir(temp_replies)\n",
    "        for j in reply_file:\n",
    "            with open(temp_replies + j) as f:\n",
    "                tweet_data[j.split('.')[0]] = (json.load(f))\n",
    "\n",
    "        lis.append(Data(structure, tweet_data, source))\n",
    "\n",
    "\n",
    "for i in lis:\n",
    "    print(i.source)\n",
    "\n",
    "\n",
    "label_path = './Datasets/semeval2017-task8-dataset/traindev/rumoureval-subtaskA-dev.json'\n",
    "\n",
    "training = {}\n",
    "\n",
    "with open(label_path,'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for i in lis:\n",
    "    for key in i.tweet_data.keys():\n",
    "        if key in labels.keys():\n",
    "            training[(key, labels[key])] = i.tweet_data[key]\n",
    "            count+=1\n",
    "            print(count, \" - \", key, \":\", labels[key])\n",
    "\n",
    "with open('dev_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(training, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile test data\n",
    "\n",
    "path = './Datasets/semeval2017-task8-test-data'\n",
    "\n",
    "lis = []\n",
    "fold = os.listdir(path)\n",
    "for i in fold:\n",
    "    if (i == '.DS_Store'):\n",
    "        continue\n",
    "    temp_source = path + '/' + i + '/source-tweet/'\n",
    "    temp_replies = path + '/' + i + '/replies/'\n",
    "    temp_struct = path + '/' + i \n",
    "    with open(temp_struct + '/structure.json') as f:\n",
    "        structure = json.load(f)\n",
    "    source_file = os.listdir(temp_source)\n",
    "    source = source_file[0].split('.')[0]\n",
    "    tweet_data = {}\t\n",
    "    with open(temp_source + source_file[0]) as f:\n",
    "        tweet_data[source] = (json.load(f))\n",
    "\n",
    "    reply_file = os.listdir(temp_replies)\n",
    "    for j in reply_file:\n",
    "        with open(temp_replies + j) as f:\n",
    "            tweet_data[j.split('.')[0]] = (json.load(f))\n",
    "\n",
    "    lis.append(Data(structure, tweet_data, source))\n",
    "\n",
    "\n",
    "for i in lis:\n",
    "    print(i.source)\n",
    "\n",
    "\n",
    "label_path = './Datasets/subtaska.json'\n",
    "\n",
    "training = {}\n",
    "\n",
    "with open(label_path,'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for i in lis:\n",
    "    for key in i.tweet_data.keys():\n",
    "        if key in labels.keys():\n",
    "            training[(key, labels[key])] = i.tweet_data[key]\n",
    "            count+=1\n",
    "            print(count, \" - \", key, \":\", labels[key])\n",
    "\n",
    "with open('gold_test_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(training, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "    # pre-processing by nltk\n",
    "# t_init = time.time()\n",
    "    \n",
    "def preprocess(filename):\n",
    "\n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #will have to add the following custom \n",
    "    custom_stopwords = set([\"http\", \"rt\", \"co\"])\n",
    "    all_stopwords = default_stopwords | custom_stopwords\n",
    "    # all_stopwords = default_stopwords\n",
    "    eng_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # eng_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "    new_dataset = {}\n",
    "    #X_preprocessed = []\n",
    "    list_all_words = []\n",
    "    #     list_all_strings = []\n",
    "    print(\"Started preprocessing!\")\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, 'rb') as handle:\n",
    "        dataset = pickle.load(handle)\n",
    "    \n",
    "    for key in dataset.keys():\n",
    "        tweet = dataset[key]\n",
    "        text = tweet['text']\n",
    "        i+=1\n",
    "        print(\"Iteration: \", i, \" - \", key[0])\n",
    "        #tokenization\n",
    "        # words = nltk.word_tokenize(text)\n",
    "        words = RegexpTokenizer('\\w+').tokenize(text)\n",
    "        #remove single character words\n",
    "        words = [word for word in words if len(word) > 1]\n",
    "        #removing numbers\n",
    "        words = [word for word in words if not word.isnumeric()]\n",
    "        #convert to lower case\n",
    "        words = [word.lower() for word in words]\n",
    "        #stem the words\n",
    "        words = [eng_stemmer.stem(word) for word in words]\n",
    "        #remove stopwords\n",
    "        words = [word for word in words if word not in all_stopwords]\n",
    "        #X_preprocessed.append(str(words))\n",
    "#         for word in words:\n",
    "#             list_all_words.append(word)\n",
    "        # f.write(\"%s\\n\" % str(words))\n",
    "        new_dataset[key] = words\n",
    "\n",
    "    #     print(\"Writing to file...\")\n",
    "\n",
    "    ## I now wish to count the most frequent words and plot them\n",
    "\n",
    "#     fdist = nltk.FreqDist(list_all_words)\n",
    "#     fdist.plot(50,cumulative=False)\n",
    "\n",
    "\n",
    "    print(\"Finished preprocessing!\")\n",
    "\n",
    "#     t_end = time.time()\n",
    "#     print(\"Finished in \", t_end-t_init, \" secs\" )\n",
    "\n",
    "    with open(filename.split(\".\")[0] + '_preproc.pickle', 'wb') as handle:\n",
    "        pickle.dump(new_dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def vectorize_data(training_file,ngram_size,gram_name):\n",
    "\n",
    "    with open(training_file, 'rb') as handle:\n",
    "        dataset = pickle.load(handle)\n",
    "\n",
    "    text = []\n",
    "\n",
    "    count_vect = CountVectorizer(ngram_range=(ngram_size, ngram_size))\n",
    "    tfidf_vect = TfidfVectorizer(ngram_range=(ngram_size, ngram_size))\n",
    "\n",
    "    for key in dataset.keys():\n",
    "        text_str = ' '.join(dataset[key])\n",
    "        text.append(text_str)\n",
    "\n",
    "    # fitting ngram model\n",
    "\n",
    "    count_vect.fit(text)\n",
    "    tfidf_vect.fit(text)\n",
    "\n",
    "    new_dataset = {}\n",
    "\n",
    "    print(\"Count vectorizer...\")\n",
    "\n",
    "    for key in dataset.keys():\n",
    "        vec = count_vect.transform([' '.join(dataset[key])])\n",
    "        # print(vec.shape)\n",
    "        new_dataset[key] = vec\n",
    "\n",
    "    with open(training_file.split('_preproc_new.')[0]+'_bow_' +gram_name+'_new.pickle', 'wb') as handle:\n",
    "        pickle.dump(new_dataset, handle)\n",
    "\n",
    "    for key in new_dataset.keys():\n",
    "        print(new_dataset[key])\n",
    "        print(new_dataset[key].shape)\n",
    "        break\n",
    "\n",
    "    new_dataset = {}\n",
    "\n",
    "    print(\"Tfidf...\")\n",
    "\n",
    "    for key in dataset.keys():\n",
    "        vec = tfidf_vect.transform([' '.join(dataset[key])])\n",
    "        # print(vec.shape)\n",
    "        new_dataset[key] = vec\n",
    "\n",
    "    with open(training_file.split('_preproc_new.')[0]+'_tfidf_' + gram_name+'_new.pickle', 'wb') as handle:\n",
    "        pickle.dump(new_dataset, handle)\n",
    "\n",
    "    for key in new_dataset.keys():\n",
    "        print(new_dataset[key])\n",
    "        print(new_dataset[key].shape)\n",
    "        break\n",
    "\n",
    "    joblib.dump(count_vect, 'bow_'+gram_name+'_new.pkl')\n",
    "    joblib.dump(tfidf_vect, 'tfidf_'+gram_name+'_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "def run_classification(model_file, train_data_file, disp_string, clf):\n",
    "\n",
    "    model = joblib.load(model_file)\n",
    "    with open(train_data_file, 'rb') as handle:\n",
    "        train = pickle.load(handle)\n",
    "\n",
    "    with open('test_labels_preproc.pickle', 'rb') as handle:\n",
    "        test = pickle.load(handle)\n",
    "\n",
    "    vec_cols = 0\n",
    "    for key in train.keys():\n",
    "        vec_cols = train[key].shape[1]\n",
    "        break\n",
    "    # print(vec_cols)\n",
    "    # print(len(test.keys()))\n",
    "    X_train = np.zeros((len(train.keys()),vec_cols),dtype=float)\n",
    "    # X_train = []\n",
    "    y_train = []\n",
    "    X_test = np.zeros((len(test.keys()),vec_cols),dtype=float)\n",
    "    # X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    # print(X_train.shape)\n",
    "    # print(X_test.shape)\n",
    "\n",
    "    mapping = {'comment':0, 'support':1, 'deny':2, 'query':3}\n",
    "\n",
    "    i = 0\n",
    "    for key in train.keys():\n",
    "        # print(X_train[i].shape)\n",
    "        # print(train[key].shape)\n",
    "        X_train[i] = train[key].toarray()\n",
    "        y_train.append(mapping[key[1]])\n",
    "        i+=1\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "    i = 0\n",
    "    for key in test.keys():\n",
    "        y_test.append(mapping[key[1]])\n",
    "        X_test[i] = model.transform([' '.join(test[key])]).toarray()\n",
    "        i+=1\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    # print(X_test)\n",
    "\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(disp_string)\n",
    "\n",
    "    print(\"Classification accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    "    target_names = ['comment', 'support', 'deny', 'query']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n",
      "For MultinomialNB, with unigram bow model: \n",
      "Classification accuracy:  0.7054337464251669\n",
      "Confusion matrix:  [[713  48  11   6]\n",
      " [ 69  23   2   0]\n",
      " [ 61   5   4   1]\n",
      " [ 99   7   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.76      0.92      0.83       778\n",
      "    support       0.28      0.24      0.26        94\n",
      "       deny       0.24      0.06      0.09        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.60      0.71      0.64      1049\n",
      "\n",
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For MultinomialNB, with bigram bow model: \n",
      "Classification accuracy:  0.7235462345090562\n",
      "Confusion matrix:  [[746  29   3   0]\n",
      " [ 81  13   0   0]\n",
      " [ 66   5   0   0]\n",
      " [104   2   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.96      0.84       778\n",
      "    support       0.27      0.14      0.18        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.72      0.64      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For MultinomialNB, with trigram bow model: \n",
      "Classification accuracy:  0.7397521448999047\n",
      "Confusion matrix:  [[771   7   0   0]\n",
      " [ 89   5   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.42      0.05      0.09        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.64      1049\n",
      "\n",
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n",
      "For MultinomialNB, with unigram tfidf model: \n",
      "Classification accuracy:  0.7387988560533841\n",
      "Confusion matrix:  [[770   8   0   0]\n",
      " [ 89   5   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.38      0.05      0.09        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.64      1049\n",
      "\n",
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For MultinomialNB, with bigram tfidf model: \n",
      "Classification accuracy:  0.7407054337464252\n",
      "Confusion matrix:  [[773   5   0   0]\n",
      " [ 90   4   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.44      0.04      0.08        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.64      1049\n",
      "\n",
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For MultinomialNB, with trigram tfidf model: \n",
      "Classification accuracy:  0.7426120114394662\n",
      "Confusion matrix:  [[778   0   0   0]\n",
      " [ 93   1   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      1.00      0.85       778\n",
      "    support       1.00      0.01      0.02        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.64      0.74      0.63      1049\n",
      "\n",
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SGDClassifier, with unigram bow model: \n",
      "Classification accuracy:  0.5938989513822688\n",
      "Confusion matrix:  [[584 101  47  46]\n",
      " [ 66  13  10   5]\n",
      " [ 47   9  14   1]\n",
      " [ 72  18   4  12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.76      0.75      0.76       778\n",
      "    support       0.09      0.14      0.11        94\n",
      "       deny       0.19      0.20      0.19        71\n",
      "      query       0.19      0.11      0.14       106\n",
      "\n",
      "avg / total       0.60      0.59      0.60      1049\n",
      "\n",
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For SGDClassifier, with bigram bow model: \n",
      "Classification accuracy:  0.7244995233555768\n",
      "Confusion matrix:  [[747  24   4   3]\n",
      " [ 81  11   2   0]\n",
      " [ 69   0   1   1]\n",
      " [102   3   0   1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.96      0.84       778\n",
      "    support       0.29      0.12      0.17        94\n",
      "       deny       0.14      0.01      0.03        71\n",
      "      query       0.20      0.01      0.02       106\n",
      "\n",
      "avg / total       0.61      0.72      0.64      1049\n",
      "\n",
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For SGDClassifier, with trigram bow model: \n",
      "Classification accuracy:  0.7397521448999047\n",
      "Confusion matrix:  [[774   3   1   0]\n",
      " [ 92   2   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.40      0.02      0.04        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.63      1049\n",
      "\n",
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n",
      "For SGDClassifier, with unigram tfidf model: \n",
      "Classification accuracy:  0.7149666348903718\n",
      "Confusion matrix:  [[728  28   7  15]\n",
      " [ 77  13   2   2]\n",
      " [ 65   0   6   0]\n",
      " [ 96   7   0   3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.94      0.83       778\n",
      "    support       0.27      0.14      0.18        94\n",
      "       deny       0.40      0.08      0.14        71\n",
      "      query       0.15      0.03      0.05       106\n",
      "\n",
      "avg / total       0.63      0.71      0.65      1049\n",
      "\n",
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For SGDClassifier, with bigram tfidf model: \n",
      "Classification accuracy:  0.7102001906577693\n",
      "Confusion matrix:  [[729  36   9   4]\n",
      " [ 81  11   2   0]\n",
      " [ 64   3   2   2]\n",
      " [ 96   6   1   3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.94      0.83       778\n",
      "    support       0.20      0.12      0.15        94\n",
      "       deny       0.14      0.03      0.05        71\n",
      "      query       0.33      0.03      0.05       106\n",
      "\n",
      "avg / total       0.62      0.71      0.64      1049\n",
      "\n",
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For SGDClassifier, with trigram tfidf model: \n",
      "Classification accuracy:  0.7340324118207817\n",
      "Confusion matrix:  [[764  12   2   0]\n",
      " [ 88   6   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.98      0.85       778\n",
      "    support       0.33      0.06      0.11        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.73      0.64      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb1 = MultinomialNB()\n",
    "run_classification('bow_unigram.pkl', 'training_labels_bow_unigram.pickle', 'For MultinomialNB, with unigram bow model: ', nb1)\n",
    "\n",
    "nb2 = MultinomialNB()\n",
    "run_classification('bow_bigram.pkl', 'training_labels_bow_bigram.pickle', 'For MultinomialNB, with bigram bow model: ', nb2)\n",
    "\n",
    "nb3 = MultinomialNB()\n",
    "run_classification('bow_trigram.pkl', 'training_labels_bow_trigram.pickle', 'For MultinomialNB, with trigram bow model: ', nb3)\n",
    "\n",
    "nb4 = MultinomialNB()\n",
    "run_classification('tfidf_unigram.pkl', 'training_labels_tfidf_unigram.pickle', 'For MultinomialNB, with unigram tfidf model: ', nb4)\n",
    "\n",
    "nb5 = MultinomialNB()\n",
    "run_classification('tfidf_bigram.pkl', 'training_labels_tfidf_bigram.pickle', 'For MultinomialNB, with bigram tfidf model: ', nb5)\n",
    "\n",
    "nb6 = MultinomialNB()\n",
    "run_classification('tfidf_trigram.pkl', 'training_labels_tfidf_trigram.pickle', 'For MultinomialNB, with trigram tfidf model: ', nb6)\n",
    "\n",
    "svm1 = linear_model.SGDClassifier()\n",
    "run_classification('bow_unigram.pkl', 'training_labels_bow_unigram.pickle', 'For SGDClassifier, with unigram bow model: ', svm1)\n",
    "\n",
    "svm2 = linear_model.SGDClassifier()\n",
    "run_classification('bow_bigram.pkl', 'training_labels_bow_bigram.pickle', 'For SGDClassifier, with bigram bow model: ', svm2)\n",
    "\n",
    "svm3 = linear_model.SGDClassifier()\n",
    "run_classification('bow_trigram.pkl', 'training_labels_bow_trigram.pickle', 'For SGDClassifier, with trigram bow model: ', svm3)\n",
    "\n",
    "svm4 = linear_model.SGDClassifier()\n",
    "run_classification('tfidf_unigram.pkl', 'training_labels_tfidf_unigram.pickle', 'For SGDClassifier, with unigram tfidf model: ', svm4)\n",
    "\n",
    "svm5 = linear_model.SGDClassifier()\n",
    "run_classification('tfidf_bigram.pkl', 'training_labels_tfidf_bigram.pickle', 'For SGDClassifier, with bigram tfidf model: ', svm5)\n",
    "\n",
    "svm6 = linear_model.SGDClassifier()\n",
    "run_classification('tfidf_trigram.pkl', 'training_labels_tfidf_trigram.pickle', 'For SGDClassifier, with trigram tfidf model: ', svm6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n",
      "For LogisticRegression, with unigram model: \n",
      "Classification accuracy:  0.7197330791229742\n",
      "Confusion matrix:  [[739  28   6   5]\n",
      " [ 79  11   2   2]\n",
      " [ 69   0   2   0]\n",
      " [ 99   4   0   3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.95      0.84       778\n",
      "    support       0.26      0.12      0.16        94\n",
      "       deny       0.20      0.03      0.05        71\n",
      "      query       0.30      0.03      0.05       106\n",
      "\n",
      "avg / total       0.62      0.72      0.64      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr1 = LogisticRegression()\n",
    "run_classification('bow_unigram.pkl', 'training_labels_bow_unigram.pickle', 'For LogisticRegression, with unigram model: ', lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For LogisticRegression, with bigram bow model: \n",
      "Classification accuracy:  0.7387988560533841\n",
      "Confusion matrix:  [[768  10   0   0]\n",
      " [ 87   7   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.41      0.07      0.13        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.64      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression()\n",
    "run_classification('bow_bigram.pkl', 'training_labels_bow_bigram.pickle', 'For LogisticRegression, with bigram bow model: ', lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For LogisticRegression, with trigram bow model: \n",
      "Classification accuracy:  0.7426120114394662\n",
      "Confusion matrix:  [[778   0   0   0]\n",
      " [ 93   1   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      1.00      0.85       778\n",
      "    support       1.00      0.01      0.02        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.64      0.74      0.63      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr3 = LogisticRegression()\n",
    "run_classification('bow_trigram.pkl', 'training_labels_bow_trigram.pickle', 'For LogisticRegression, with trigram bow model: ', lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 6718) (4238,)\n",
      "(1049, 6718) (1049,)\n",
      "For LogisticRegression, with unigram tfidf model: \n",
      "Classification accuracy:  0.7340324118207817\n",
      "Confusion matrix:  [[762  15   0   1]\n",
      " [ 86   8   0   0]\n",
      " [ 70   1   0   0]\n",
      " [104   2   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.75      0.98      0.85       778\n",
      "    support       0.31      0.09      0.13        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.73      0.64      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 24619) (4238,)\n",
      "(1049, 24619) (1049,)\n",
      "For LogisticRegression, with bigram tfidf model: \n",
      "Classification accuracy:  0.7359389895138226\n",
      "Confusion matrix:  [[766  12   0   0]\n",
      " [ 88   6   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.98      0.85       778\n",
      "    support       0.33      0.06      0.11        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.74      0.64      1049\n",
      "\n",
      "(4238, 24644) (4238,)\n",
      "(1049, 24644) (1049,)\n",
      "For LogisticRegression, with trigram tfidf model: \n",
      "Classification accuracy:  0.7416587225929456\n",
      "Confusion matrix:  [[778   0   0   0]\n",
      " [ 94   0   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      1.00      0.85       778\n",
      "    support       0.00      0.00      0.00        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.55      0.74      0.63      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr4 = LogisticRegression()\n",
    "run_classification('tfidf_unigram.pkl', 'training_labels_tfidf_unigram.pickle', 'For LogisticRegression, with unigram tfidf model: ', lr4)\n",
    "\n",
    "lr5 = LogisticRegression()\n",
    "run_classification('tfidf_bigram.pkl', 'training_labels_tfidf_bigram.pickle', 'For LogisticRegression, with bigram tfidf model: ', lr5)\n",
    "\n",
    "lr6 = LogisticRegression()\n",
    "run_classification('tfidf_trigram.pkl', 'training_labels_tfidf_trigram.pickle', 'For LogisticRegression, with trigram tfidf model: ', lr6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 103) (4238,)\n",
      "(1049, 103) (1049,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('train_data_w2v.npy')\n",
    "y_train = np.load('train_label_w2v.npy')\n",
    "X_test = np.load('test_data_w2v.npy')\n",
    "y_test = np.load('test_label_w2v.npy')\n",
    "        \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.771210676835081\n",
      "Confusion matrix:  [[ 23   3   0  68]\n",
      " [  0  35   0  71]\n",
      " [  1   2   0  68]\n",
      " [  8  18   1 751]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    support       0.72      0.24      0.37        94\n",
      "      query       0.60      0.33      0.43       106\n",
      "       deny       0.00      0.00      0.00        71\n",
      "    comment       0.78      0.97      0.87       778\n",
      "\n",
      "avg / total       0.71      0.77      0.72      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Classification accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    "target_names = ['support', 'query','deny','comment']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.7130600571973308\n",
      "Confusion matrix:  [[ 29   4   1  60]\n",
      " [  4  24   3  75]\n",
      " [  3   4   1  63]\n",
      " [ 53  23   8 694]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    support       0.33      0.31      0.32        94\n",
      "      query       0.44      0.23      0.30       106\n",
      "       deny       0.08      0.01      0.02        71\n",
      "    comment       0.78      0.89      0.83       778\n",
      "\n",
      "avg / total       0.66      0.71      0.68      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Classification accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    "target_names = ['support', 'query','deny','comment']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.7273593898951383\n",
      "Confusion matrix:  [[ 30   2   0  62]\n",
      " [  3  24   0  79]\n",
      " [  4   1   2  64]\n",
      " [ 45  21   5 707]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    support       0.37      0.32      0.34        94\n",
      "      query       0.50      0.23      0.31       106\n",
      "       deny       0.29      0.03      0.05        71\n",
      "    comment       0.78      0.91      0.84       778\n",
      "\n",
      "avg / total       0.68      0.73      0.69      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = RandomForestClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"Classification accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix: \", confusion_matrix(y_test, y_pred))\n",
    "target_names = ['support', 'query','deny','comment']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 103) (4238,)\n",
      "(1049, 103) (1049,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('train_data_w2v.npy')\n",
    "y_train = np.load('train_label_w2v.npy')\n",
    "X_test = np.load('test_data_w2v.npy')\n",
    "y_test = np.load('test_label_w2v.npy')\n",
    "        \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #will have to add the following custom \n",
    "    custom_stopwords = set([\"http\", \"rt\", \"co\"])\n",
    "    stops = default_stopwords | custom_stopwords\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "#     stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "    # pre-processing by nltk\n",
    "\n",
    "def preprocess_data(input_file):\n",
    "\n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #will have to add the following custom \n",
    "    custom_stopwords = set([\"http\", \"rt\", \"co\"])\n",
    "    all_stopwords = default_stopwords | custom_stopwords\n",
    "    # all_stopwords = default_stopwords\n",
    "    eng_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # eng_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "    new_dataset = {}\n",
    "    #X_preprocessed = []\n",
    "    list_all_words = []\n",
    "    #     list_all_strings = []\n",
    "    print(\"Started preprocessing \" + input_file + \" !\")\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    with open(input_file, 'rb') as handle:\n",
    "        dataset = pickle.load(handle)\n",
    "\n",
    "    df = pd.DataFrame({'tweet':[], 'label':[]})\n",
    "    mapping = {'comment':0, 'support':1, 'deny':2, 'query':3}\n",
    "\n",
    "    i = 0\n",
    "    for key in dataset.keys():\n",
    "    #     print(key, dataset[key])\n",
    "        df.loc[i] = {'tweet':clean_text(dataset[key]['text']), 'label':mapping[key[1]]}\n",
    "        i+=1\n",
    "\n",
    "    #     print(\"Writing to file...\")\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df.to_csv(input_file.split(\".\")[0]+'_preproc.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preprocessing training_labels.pickle !\n",
      "      label                                              tweet\n",
      "0       1.0  pari manhunt : charliehebdo attack ak - 47 rif...\n",
      "1       0.0  rt com hope french get foreign legion ass put ...\n",
      "2       0.0                 rt com gign got work cut today lol\n",
      "3       0.0                              rt com better grammar\n",
      "4       0.0        rt com + islamispeac http : t co lhhixcswt9\n",
      "5       0.0       rt com cnn msnbc foxnew hide msnbc cnn offic\n",
      "6       0.0  gargirawat thank repli mam realli appreci twee...\n",
      "7       0.0  rt com islam barbar heinous religion ca not re...\n",
      "8       0.0                         rt com crazi jew ! sarcasm\n",
      "9       0.0  gargirawat mam get noopurtiwari mam contact nu...\n",
      "10      0.0                   bhavdeepnama it tri noopurtiwari\n",
      "11      0.0                                    rt com mean rpg\n",
      "12      0.0  scari ! rt com charliehebdo attack ak - 47 rif...\n",
      "13      0.0   gargirawat eas weapon brandish threaten scenario\n",
      "14      0.0         rt com franc pay big price puppet american\n",
      "15      0.0  gargirawat rt com come on ! sure scari rss hin...\n",
      "16      0.0  rt com aljazeera report rocket launcher edit link\n",
      "17      1.0  franc : peopl dead shoot satir week newspap ch...\n",
      "18      3.0  euronew tradedesk steve french crime passion a...\n",
      "19      0.0  nanospawn socialist antisemit anti zionist usu...\n",
      "20      0.0  euronew franc : dead shoot satir week charlieh...\n",
      "21      0.0  euronew lol million muslim franc disgrac frenc...\n",
      "22      2.0  j0nathandavi who stupid partial opinion like o...\n",
      "23      1.0  french polic publish photo suspect yesterday m...\n",
      "24      0.0                      julienpain france24 face evil\n",
      "25      1.0  alqaedayemen + charliehebdo connexion; policew...\n",
      "26      1.0  julienpain french polic publish photo suspect ...\n",
      "27      1.0  report : charliehebdo suspect kill http : t co...\n",
      "28      0.0                     huffingtonpost that good start\n",
      "29      0.0  huffingtonpost great news ! trial taxpay money...\n",
      "...     ...                                                ...\n",
      "4208    1.0                        adapttor batchelorshow yeah\n",
      "4209    2.0  adapttor batchelorshow keep mind unconfirm rum...\n",
      "4210    0.0  shaunynew batchelorshow hope got hold american...\n",
      "4211    3.0                               shaunynew get rumour\n",
      "4212    0.0             shaunynew batchelorshow die childbirth\n",
      "4213    1.0  taw3333 emm never tag user video grace shaunyn...\n",
      "4214    0.0  captaincorri mani sourc tube vladimir putin + ...\n",
      "4215    1.0  hoppla ! l0gg0l : swiss rumor : putin absenc d...\n",
      "4216    0.0  russian market l0gg0l given state russian medi...\n",
      "4217    0.0             russian market l0gg0l cours keep dream\n",
      "4218    1.0  russian market : swiss rumor : putin absenc du...\n",
      "4219    0.0  russian market christophheer52 l0gg0l money st...\n",
      "4220    0.0  russian market l0gg0l putin build nation great...\n",
      "4221    0.0         russian market ferrotv l0gg0l welcom sir !\n",
      "4222    3.0  russian market read this http : t co p5s1ug5b9...\n",
      "4223    0.0  russian market l0gg0l make sens though switzer...\n",
      "4224    0.0   russian market l0gg0l least merkel would problem\n",
      "4225    0.0  prove flexibl ! russian market christophheer52...\n",
      "4226    0.0  appear origin sourc putin kabayeva swiss clini...\n",
      "4227    3.0  bbcdaniel swiss clinic give birth russia miss ...\n",
      "4228    0.0    bbcdaniel difficult think rosemari babi instanc\n",
      "4229    1.0  bbcdaniel + what make think babi stori may tru...\n",
      "4230    0.0                 bbcdaniel plus direct info field !\n",
      "4231    1.0  bbcdaniel ellenbarrynyt well blick tabloid nec...\n",
      "4232    1.0  coup jimgeraghti : rumor russian militari atta...\n",
      "4233    0.0                         irishspi jimgeraghti reset\n",
      "4234    0.0  karmacamilleon1 meme maker let this ! see red ...\n",
      "4235    3.0   irishspi jimgeraghti prepar launch yet anoth war\n",
      "4236    2.0  irishspi russian emb deni russianembassi min +...\n",
      "4237    0.0  karmacamilleon1 irishspi jimgeraghti http : t ...\n",
      "\n",
      "[4238 rows x 2 columns]\n",
      "Started preprocessing test_labels.pickle !\n",
      "      label                                              tweet\n",
      "0       1.0  appal attack charli hebdo pari probabl journal...\n",
      "1       0.0  tnewtondunn solidar key ca not mute face repul...\n",
      "2       0.0  olenkafrenkiel cavelloman1 mjhsinclair tnewton...\n",
      "3       0.0  m33ryg tnewtondunn mehdirhasan suppli factual ...\n",
      "4       0.0  mjhsinclair olenkafrenkiel tnewtondunn no on e...\n",
      "5       0.0  mumbobe ppl understand muslim get hurt prophet...\n",
      "6       0.0  katherine1924 mumbobe tnewtondunn mehdirhasan ...\n",
      "7       0.0  tnewtondunn guy put one cartoon mayb last one ...\n",
      "8       1.0  tnewtondunn freedom speech someth fought for 1...\n",
      "9       0.0  unbiasedf fact long stori keep short 1 6 bil p...\n",
      "10      0.0                      tnewtondunn theuneasyreap rip\n",
      "11      0.0  mjhsinclair tnewtondunn sad sinc none press da...\n",
      "12      0.0  unbiasedf tnewtondunn mehdirhasan cours allow ...\n",
      "13      0.0  m33ryg katherine1924 tnewtondunn mehdirhasan p...\n",
      "14      0.0             tnewtondunn theuneasyreap poor peopl :\n",
      "15      0.0  tnewtondunn import moment paper could provid s...\n",
      "16      0.0  m33ryg tnewtondunn mehdirhasan cours free spee...\n",
      "17      0.0  tnewtondunn mehdirhasan attack religion free s...\n",
      "18      0.0  m33ryg tnewtondunn mehdirhasan fact religion s...\n",
      "19      0.0  m33ryg tnewtondunn mehdirhasan think peopl all...\n",
      "20      0.0               tnewtondunn toadmeist + when wake up\n",
      "21      0.0  mumbobe tnewtondunn mehdirhasan free speech hu...\n",
      "22      0.0  m33ryg mumbobe tnewtondunn mehdirhasan arsehol...\n",
      "23      0.0  m33ryg katherine1924 mumbobe tnewtondunn mehdi...\n",
      "24      0.0  cavelloman1 mjhsinclair tnewtondunn victoriape...\n",
      "25      3.0  katherine1924 mumbobe tnewtondunn mehdirhasan ...\n",
      "26      0.0                     tnewtondunn i will ridewithyou\n",
      "27      0.0  m33ryg mumbobe tnewtondunn mehdirhasan presum ...\n",
      "28      1.0  germanw co - pilot serious depress episod : bi...\n",
      "29      0.0         reuter mass media becom cancer sell news !\n",
      "...     ...                                                ...\n",
      "1019    0.0  asamjulian look like candl strong enough hold ...\n",
      "1020    3.0                                 asamjulian minivan\n",
      "1021    0.0                  asamjulian https : t co rhd3ewpyg\n",
      "1022    0.0   asamjulian lyndag1963 day what wonder trip would\n",
      "1023    0.0                      asamjulian bonanof crack kill\n",
      "1024    0.0                    asamjulian jgreen46 parkinson !\n",
      "1025    0.0  asamjulian bberghofsr look like depend might full\n",
      "1026    0.0  cherylann729 asamjulian jgreen46 hillari ill d...\n",
      "1027    0.0  that true notic befor primmr : asamjulian look...\n",
      "1028    1.0  asamjulian that special made car step her poor...\n",
      "1029    0.0  deerman118 asamjulian bberghofsr guy pretti ju...\n",
      "1030    0.0                 asamjulian https : t co oj0yvvchd0\n",
      "1031    0.0  asamjulian ca not get car without help ca not ...\n",
      "1032    0.0  plewis1956 primmr noblefact asamjulian run sen...\n",
      "1033    0.0  plewis1956 primmr noblefact asamjulian report ...\n",
      "1034    0.0  asamjulian theivanvolt get bitch car kane let ...\n",
      "1035    0.0                           asamjulian grip aw tight\n",
      "1036    0.0             cherylann729 asamjulian jgreen46 right\n",
      "1037    1.0  whitejacketpink asamjulian jgreen46 agre robin...\n",
      "1038    0.0  atrump sue51684 asamjulian lol hilari troubl p...\n",
      "1039    0.0  primmr noblefact asamjulian escalad notori low...\n",
      "1040    0.0  asamjulian tri hilari grandma love it https : ...\n",
      "1041    0.0    asamjulian step vehicl rose inch fat head big !\n",
      "1042    1.0  asamjulian move slower old mom did someth wron...\n",
      "1043    0.0                           asamjulian need nap bill\n",
      "1044    1.0  asamjulian unfit offic physic moral hillarycli...\n",
      "1045    0.0  asamjulian ashes2birth hillari love media tri ...\n",
      "1046    1.0  asamjulian abusedtaxpay look like woman hard t...\n",
      "1047    0.0  asamjulian pink lady56 fall pleas god fall mak...\n",
      "1048    0.0  asamjulian michelleaukamp hillaryclinton hilla...\n",
      "\n",
      "[1049 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_data('training_labels.pickle')\n",
    "test_df = preprocess_data('test_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 5)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "MAXLEN = 10\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_df['tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(train_df['tweet'])\n",
    "X_train = pad_sequences(sequences, maxlen=MAXLEN)\n",
    "y_train = np.array(train_df['label'].tolist())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2542 samples, validate on 1696 samples\n",
      "Epoch 1/20\n",
      "2542/2542 [==============================] - 4s 2ms/step - loss: 0.6778 - acc: 0.2266 - val_loss: 0.6642 - val_acc: 0.2229\n",
      "Epoch 2/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: 0.3535 - acc: 0.5551 - val_loss: 0.9359 - val_acc: 0.3833\n",
      "Epoch 3/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -1.0421 - acc: 0.6452 - val_loss: 1.1408 - val_acc: 0.4180\n",
      "Epoch 4/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -1.8277 - acc: 0.7156 - val_loss: 1.8590 - val_acc: 0.4080\n",
      "Epoch 5/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.1988 - acc: 0.7486 - val_loss: 2.0758 - val_acc: 0.4062\n",
      "Epoch 6/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.4754 - acc: 0.7624 - val_loss: 2.0816 - val_acc: 0.4175\n",
      "Epoch 7/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.6139 - acc: 0.7734 - val_loss: 1.8672 - val_acc: 0.4298\n",
      "Epoch 8/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.7304 - acc: 0.7852 - val_loss: 1.9150 - val_acc: 0.4381\n",
      "Epoch 9/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.7630 - acc: 0.7891 - val_loss: 2.3734 - val_acc: 0.4334\n",
      "Epoch 10/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.7725 - acc: 0.7860 - val_loss: 2.2040 - val_acc: 0.4198\n",
      "Epoch 11/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.9197 - acc: 0.7943 - val_loss: 2.2456 - val_acc: 0.4304\n",
      "Epoch 12/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -2.9816 - acc: 0.8002 - val_loss: 2.3955 - val_acc: 0.4346\n",
      "Epoch 13/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.0236 - acc: 0.8084 - val_loss: 2.3369 - val_acc: 0.4428\n",
      "Epoch 14/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.0302 - acc: 0.8108 - val_loss: 2.2836 - val_acc: 0.4505\n",
      "Epoch 15/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.0637 - acc: 0.8080 - val_loss: 2.2887 - val_acc: 0.4564\n",
      "Epoch 16/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.0912 - acc: 0.8183 - val_loss: 2.4140 - val_acc: 0.4499\n",
      "Epoch 17/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.1023 - acc: 0.8143 - val_loss: 2.5962 - val_acc: 0.4522\n",
      "Epoch 18/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.0814 - acc: 0.8151 - val_loss: 2.4285 - val_acc: 0.4534\n",
      "Epoch 19/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.1017 - acc: 0.8179 - val_loss: 2.5100 - val_acc: 0.4558\n",
      "Epoch 20/20\n",
      "2542/2542 [==============================] - 3s 1ms/step - loss: -3.1039 - acc: 0.8159 - val_loss: 2.4689 - val_acc: 0.4570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93be27cda0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=MAXLEN))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.4, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1049, 5)\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_df['tweet'])\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAXLEN)\n",
    "y_test = np.array(test_df['label'].tolist())\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049/1049 [==============================] - 1s 944us/step\n",
      "Test score: 2.202318824365104\n",
      "Test accuracy: 0.5309818875119161\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 5, 100)            2000000   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,080,501\n",
      "Trainable params: 2,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "1049/1049 [==============================] - 0s 454us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.77      0.66      0.71       778\n",
      "    support       0.12      0.48      0.19        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.53      0.54      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "target_names = ['comment', 'support', 'deny', 'query']\n",
    "y_pred = model.predict_classes(X_test, verbose=1)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id                                              tweet\n",
      "0      1060611565945151488  RT @CommonSenseWise: Be it #Californiashooting...\n",
      "1      1060611559616065536  RT @JamesMelville: More Americans have died fr...\n",
      "2      1060611556084465664  RT @DBolelli: mass shootings are the canary in...\n",
      "3      1060611555870556160  RT @MeanwhileinCana: Prayers don't stop mass s...\n",
      "4      1060611552548610048  RT @invisibleman_17: #CaliforniaShooting #Cali...\n",
      "5      1060611551479119873  RT @JamesMelville: More Americans have died fr...\n",
      "6      1060611547771351040  RT @USATODAY: \"Please pray if you believe....p...\n",
      "7      1060611545535664128  RT @AynRandPaulRyan: Hey, @jack - Any chance @...\n",
      "8      1060611540326465536  RT @sjredmond: Just a reminder that 13 months ...\n",
      "9      1060611538581667841  RT @AynRandPaulRyan: Hey, @jack - Any chance @...\n",
      "10     1060611535607783424  RT @NBCDFW: Jason Coffman speaks to the media ...\n",
      "11     1060611534441852930  RT @AmyMek: Thousands of people lined the stre...\n",
      "12     1060611532793491459  RT @sjredmond: Just a reminder that 13 months ...\n",
      "13     1060611531287814147  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "14     1060611530616492032  RT @Millie__Weaver: Liberals are already calli...\n",
      "15     1060611530025246721  RT @JimmyREMfan: #Californiashooting \\n\\nLet m...\n",
      "16     1060611525658984448  RT @tictoc: Ventura County Sheriff Geoff Dean ...\n",
      "17     1060611517769408512  @Zach_rjmlab Just look at #CaliforniaShooting ...\n",
      "18     1060611515634630656  RT @JohnDoe_997: \"If a guy had a gun, maybe He...\n",
      "19     1060611511947681792  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "20     1060611504662241280  RT @AynRandPaulRyan: This is devastating. I ca...\n",
      "21     1060611504431603712  RT @tariqnasheed: This Thousand Oaks #Californ...\n",
      "22     1060611503848677381  Thoughts and prayers don't save police officer...\n",
      "23     1060611503127232512  Prayers don't stop mass shootings, sensible gu...\n",
      "24     1060611503043371008  RT @AynRandPaulRyan: Hey, @jack - Any chance @...\n",
      "25     1060611491693395968  RT @JamesMelville: More Americans have died fr...\n",
      "26     1060611490972000256  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "27     1060611488505708544  RT @AmyMek: Thousands of people lined the stre...\n",
      "28     1060611481086025728  RT @AynRandPaulRyan: Hey, @jack - Any chance @...\n",
      "29     1060611480276488192  Looking for\" OR \"are seeking\" OR \"is seeking\" ...\n",
      "...                    ...                                                ...\n",
      "14970  1060566240262082567  So horrible a lot of young lives taken again, ...\n",
      "14971  1060566234117267456  RT @michaelbeatty3: \"I know where I'm going wh...\n",
      "14972  1060566230065704960  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "14973  1060566226311806976  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "14974  1060566225934270464  RT @Millie__Weaver: Liberals are already calli...\n",
      "14975  1060566225871351808  Ian David Long does not appear to be:\\n\\nBlack...\n",
      "14976  1060566223119884288  RT @AmandiOnAir: Wait a minute, \"thoughts &amp...\n",
      "14977  1060566218967564288  RT @PhilippaDanso: Immediately after the #Cali...\n",
      "14978  1060566218774466561  #Californiashooting #AmericanHorrorStory #GunC...\n",
      "14979  1060566217205919744  RT @jamietworkowski: 12 innocent people were k...\n",
      "14980  1060566217105129472  RT @kharyp: you never think it's gonna be you ...\n",
      "14981  1060566214416584706  RT @stereofiasco: This has been the deadliest ...\n",
      "14982  1060566211853979653  #CaliforniaShooting\\nAmerica, the land where g...\n",
      "14983  1060566210293784576  RT @sjredmond: Just a reminder that 13 months ...\n",
      "14984  1060566205579386882  RT @RobWilliamsCTV: Today is the 312th day of ...\n",
      "14985  1060566200730689536  @GOP @The_RGA @GovernorDeal @BrianKempGA #Exac...\n",
      "14986  1060566198482538496  RT @Millie__Weaver: Liberals are already calli...\n",
      "14987  1060566196108566528  RT @JohnDoe_997: \"If a guy had a gun, maybe He...\n",
      "14988  1060566195584159744  RT @AmandiOnAir: Wait a minute, \"thoughts &amp...\n",
      "14989  1060566195198459904  Theres been 318 days in 2018 and so far 307 m...\n",
      "14990  1060566191801057280  RT @Marina_Sirtis: So do we need armed guards ...\n",
      "14991  1060566190383226881  RT @RobJBye: As much as I want to move to Amer...\n",
      "14992  1060566185547309056  Heartbroken to hear the California mass shooti...\n",
      "14993  1060566184544878592  RT @TravelGoC: #CaliforniaShooting Canadians r...\n",
      "14994  1060566178605735942  RT @DeNaakteKiezer: @realDonaldTrump For sure ...\n",
      "14995  1060566178333110272  RT @JimmyREMfan: #Californiashooting \\n\\nLet m...\n",
      "14996  1060566177787842561  RT @crimsonfaith88: Wow! #California #Califo...\n",
      "14997  1060566176110166016  RT @standbyshane: My best friend is okay and s...\n",
      "14998  1060566171664113664  another mass shooting, prayers out to all the ...\n",
      "14999  1060566169835528192  RT @AngeloJohnGage: Here's the truth about the...\n",
      "\n",
      "[15000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "df = pd.DataFrame({'tweet':[], 'id':[]})\n",
    "i = 0\n",
    "\n",
    "ids = []\n",
    "with open('tweets_california_shootout.json','r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['id'] in ids:\n",
    "            continue\n",
    "        ids.append(data['id'])\n",
    "        df.loc[i] = {'tweet':data['text'], 'id':data['id_str']}\n",
    "        i+=1\n",
    "#         if i == 1000:\n",
    "#             break\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_california_shootout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def read_new_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df = df[df['label'].notnull()]\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df_wrong_label = df[(df['label'] != 'deny') & (df['label'] != 'query')]\n",
    "\n",
    "    # print(df_wrong_label)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                   id  \\\n",
      "35           35  1060610910639710208   \n",
      "38           38  1060610656364322816   \n",
      "57           57  1060609258247487488   \n",
      "63           63  1060608835277254658   \n",
      "74           74  1060607105407836160   \n",
      "92           92  1060605006754299904   \n",
      "97           97  1060604191851212800   \n",
      "102         102  1060602955454386176   \n",
      "105         105  1060602434811240454   \n",
      "107         107  1060602375717707776   \n",
      "108         108  1060602261489897472   \n",
      "110         110  1060602072259792897   \n",
      "115         115  1060601273387569152   \n",
      "120         120  1060599267394412545   \n",
      "132         132  1060596374704701443   \n",
      "153         153  1060591703550246912   \n",
      "168         168  1060589830646652928   \n",
      "169         169  1060589539872436224   \n",
      "172         172  1060589020814684163   \n",
      "176         176  1060588604626472960   \n",
      "190         190  1060586450578681857   \n",
      "208         208  1060583198730665985   \n",
      "212         212  1060582857251348482   \n",
      "213         213  1060582828331687938   \n",
      "228         228  1060579899021385729   \n",
      "280         280  1060574504538251264   \n",
      "286         286  1060574240204820480   \n",
      "296         296  1060573431090110465   \n",
      "330         330  1060569320697131008   \n",
      "345         345  1060567962229268481   \n",
      "348         348  1060567558586216448   \n",
      "351         351  1060567470136676352   \n",
      "352         352  1060567298178760706   \n",
      "407         407  1060563390534467584   \n",
      "485         485  1060558904290144256   \n",
      "592         592  1060554141951578115   \n",
      "602         602  1060553598600404993   \n",
      "763         763  1060544939740655616   \n",
      "819         819  1060543190506397696   \n",
      "876         876  1060541812534845440   \n",
      "\n",
      "                                                 tweet  label  \n",
      "35   @realDonaldTrump What are you going to do abou...  query  \n",
      "38   RT @QuestiGirl: When do things become a mathem...  query  \n",
      "57   @NRA Can gun violence *really* be a solvable p...  query  \n",
      "63   RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "74   When do things become a mathematical impossibi...  query  \n",
      "92   RT @GematriaClub: Another #DeepState + #FakeNe...   deny  \n",
      "97   Another #DeepState + #FakeNews False Flag Even...   deny  \n",
      "102  RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "105  @HamillHimself How. Many. Will. It. Take?\\nHow...  query  \n",
      "107  Again...why are they taking out #LasVegasShoot...  query  \n",
      "108  RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "110  Chances you are at a mass shooting is slim but...  query  \n",
      "115  @MarshaBlackburn Are you kidding me with this?...  query  \n",
      "120  RT @LauraLoomer: ISIS claimed responsibility f...   deny  \n",
      "132  @Breaking911 @diana102252 #CIA did this, #LasV...   deny  \n",
      "153  What are the odds? Seriously, what are the odd...  query  \n",
      "168  @AbbyMartin What are the chances that people i...  query  \n",
      "169  We will NEVER find out what really happened......   deny  \n",
      "172  RT @LorrieNJ50: Research. Who inside the bar w...  query  \n",
      "176  Enjoy your Thoughts and Prayers America. \\nAga...   deny  \n",
      "190  No. There's NO #DeepStateStink to this at all...   deny  \n",
      "208                Still no truth on #LasVegasShooting   deny  \n",
      "212  And we are to believe that this is totally ran...  query  \n",
      "213  RT @pitsr4me: Hmm  Are you putting two and tw...  query  \n",
      "228  What are the odds that anyone survives one mas...  query  \n",
      "280  RT @pitsr4me: Hmm  Are you putting two and tw...  query  \n",
      "286  Research. Who inside the bar were also witness...  query  \n",
      "296  Hmm  Are you putting two and two together\\n...  query  \n",
      "330  RT @LauraLoomer: ISIS claimed responsibility f...   deny  \n",
      "345  RT @taxcutsforall: What are the odds?  When is...  query  \n",
      "348  RT @sovereignity77: WHAT! #LasVegasShooting su...  query  \n",
      "351  WHAT! #LasVegasShooting survivors attacked aga...  query  \n",
      "352  \"Las Vegas\" is trending on Twitter, because of...  query  \n",
      "407  @sharonfromNOLA Did you fully read the report...   deny  \n",
      "485  And there it is... Two days after dems take th...  query  \n",
      "592  RT @LauraLoomer: 12 were killed in a Californi...  query  \n",
      "602  Do you remember the #LasVegasShooting? https:/...  query  \n",
      "763  @EdKrassen @Real_ImoGirl Irony: it has been re...   deny  \n",
      "819  RT @LauraLoomer: Yup. Just like the #LasVegasS...   deny  \n",
      "876  I'm so sorry for your loss. \\n\\nIt's ridiculo...   deny  \n",
      "40\n",
      "     Unnamed: 0            id  \\\n",
      "13           13  1.060612e+18   \n",
      "36           36  1.060611e+18   \n",
      "48           48  1.060611e+18   \n",
      "52           52  1.060611e+18   \n",
      "63           63  1.060611e+18   \n",
      "68           68  1.060611e+18   \n",
      "92           92  1.060611e+18   \n",
      "107         107  1.060611e+18   \n",
      "110         110  1.060611e+18   \n",
      "111         111  1.060611e+18   \n",
      "112         112  1.060611e+18   \n",
      "117         117  1.060611e+18   \n",
      "118         118  1.060611e+18   \n",
      "131         131  1.060611e+18   \n",
      "132         132  1.060611e+18   \n",
      "133         133  1.060611e+18   \n",
      "134         134  1.060611e+18   \n",
      "142         142  1.060611e+18   \n",
      "174         174  1.060611e+18   \n",
      "207         207  1.060611e+18   \n",
      "\n",
      "                                                 tweet  label  \n",
      "13   RT @Marina_Sirtis: So do we need armed guards ...  query  \n",
      "36   RT @themiamivoice: It's just another mass shoo...   deny  \n",
      "48   RT @AngelaBelcamino: Answer this... \\n\\nIf you...  query  \n",
      "52   RT @dave__whiteside: Thoughts on yet another m...   deny  \n",
      "63   RT @sjredmond: Why has gun violence research h...  query  \n",
      "68   RT @BettyBowers: Guns don't kill people?  Well...  query  \n",
      "92   RT @austinjrandall: two days ago, we had the o...  query  \n",
      "107  RT @CommonSenseWise: Be it #Californiashooting...   deny  \n",
      "110  RT @AynRandPaulRyan: According to Republicans,...   deny  \n",
      "111  RT @Harryslaststand: The last thing America ne...   deny  \n",
      "112  RT @nachi316: He doesnt look black.\\nHe sures...   deny  \n",
      "117  RT @taradublinrocks: Why does @PrisonPlanet st...  query  \n",
      "118  RT @austin_sam001: Oh, so there was another ma...  query  \n",
      "131  RT @kharyp: Mental illnesses didnt shoot &amp...   deny  \n",
      "132  RT @KingAtlasLIVE: Blaming the NRA for gun vio...   deny  \n",
      "133  RT @AndyOstroy: Another shooting by a white ma...   deny  \n",
      "134  RT @taradublinrocks: Why does @PrisonPlanet st...  query  \n",
      "142  @ABC This right here, makes it very clear to m...  query  \n",
      "174  What the hell is happening when our country su...  query  \n",
      "207  another shooting in the states? when does it e...  query  \n",
      "20\n"
     ]
    }
   ],
   "source": [
    "lasvegas = read_new_data('tweets_lasvegas_shootout_labeled.csv')\n",
    "# df.to_csv('las_vegas_labeled.csv')\n",
    "california = read_new_data('tweets_california_shootout_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index  Unnamed: 0            id  \\\n",
      "0      35          35  1.060611e+18   \n",
      "1      38          38  1.060611e+18   \n",
      "2      57          57  1.060609e+18   \n",
      "3      63          63  1.060609e+18   \n",
      "4      74          74  1.060607e+18   \n",
      "5      92          92  1.060605e+18   \n",
      "6      97          97  1.060604e+18   \n",
      "7     102         102  1.060603e+18   \n",
      "8     105         105  1.060602e+18   \n",
      "9     107         107  1.060602e+18   \n",
      "10    108         108  1.060602e+18   \n",
      "11    110         110  1.060602e+18   \n",
      "12    115         115  1.060601e+18   \n",
      "13    120         120  1.060599e+18   \n",
      "14    132         132  1.060596e+18   \n",
      "15    153         153  1.060592e+18   \n",
      "16    168         168  1.060590e+18   \n",
      "17    169         169  1.060590e+18   \n",
      "18    172         172  1.060589e+18   \n",
      "19    176         176  1.060589e+18   \n",
      "20    190         190  1.060586e+18   \n",
      "21    208         208  1.060583e+18   \n",
      "22    212         212  1.060583e+18   \n",
      "23    213         213  1.060583e+18   \n",
      "24    228         228  1.060580e+18   \n",
      "25    280         280  1.060575e+18   \n",
      "26    286         286  1.060574e+18   \n",
      "27    296         296  1.060573e+18   \n",
      "28    330         330  1.060569e+18   \n",
      "29    345         345  1.060568e+18   \n",
      "30    348         348  1.060568e+18   \n",
      "31    351         351  1.060567e+18   \n",
      "32    352         352  1.060567e+18   \n",
      "33    407         407  1.060563e+18   \n",
      "34    485         485  1.060559e+18   \n",
      "35    592         592  1.060554e+18   \n",
      "36    602         602  1.060554e+18   \n",
      "37    763         763  1.060545e+18   \n",
      "38    819         819  1.060543e+18   \n",
      "39    876         876  1.060542e+18   \n",
      "40     13          13  1.060612e+18   \n",
      "41     36          36  1.060611e+18   \n",
      "42     48          48  1.060611e+18   \n",
      "43     52          52  1.060611e+18   \n",
      "44     63          63  1.060611e+18   \n",
      "45     68          68  1.060611e+18   \n",
      "46     92          92  1.060611e+18   \n",
      "47    107         107  1.060611e+18   \n",
      "48    110         110  1.060611e+18   \n",
      "49    111         111  1.060611e+18   \n",
      "50    112         112  1.060611e+18   \n",
      "51    117         117  1.060611e+18   \n",
      "52    118         118  1.060611e+18   \n",
      "53    131         131  1.060611e+18   \n",
      "54    132         132  1.060611e+18   \n",
      "55    133         133  1.060611e+18   \n",
      "56    134         134  1.060611e+18   \n",
      "57    142         142  1.060611e+18   \n",
      "58    174         174  1.060611e+18   \n",
      "59    207         207  1.060611e+18   \n",
      "\n",
      "                                                tweet  label  \n",
      "0   @realDonaldTrump What are you going to do abou...  query  \n",
      "1   RT @QuestiGirl: When do things become a mathem...  query  \n",
      "2   @NRA Can gun violence *really* be a solvable p...  query  \n",
      "3   RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "4   When do things become a mathematical impossibi...  query  \n",
      "5   RT @GematriaClub: Another #DeepState + #FakeNe...   deny  \n",
      "6   Another #DeepState + #FakeNews False Flag Even...   deny  \n",
      "7   RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "8   @HamillHimself How. Many. Will. It. Take?\\nHow...  query  \n",
      "9   Again...why are they taking out #LasVegasShoot...  query  \n",
      "10  RT @WVWOnline: Chances you are at a mass shoot...  query  \n",
      "11  Chances you are at a mass shooting is slim but...  query  \n",
      "12  @MarshaBlackburn Are you kidding me with this?...  query  \n",
      "13  RT @LauraLoomer: ISIS claimed responsibility f...   deny  \n",
      "14  @Breaking911 @diana102252 #CIA did this, #LasV...   deny  \n",
      "15  What are the odds? Seriously, what are the odd...  query  \n",
      "16  @AbbyMartin What are the chances that people i...  query  \n",
      "17  We will NEVER find out what really happened......   deny  \n",
      "18  RT @LorrieNJ50: Research. Who inside the bar w...  query  \n",
      "19  Enjoy your Thoughts and Prayers America. \\nAga...   deny  \n",
      "20  No. There's NO #DeepStateStink to this at all...   deny  \n",
      "21                Still no truth on #LasVegasShooting   deny  \n",
      "22  And we are to believe that this is totally ran...  query  \n",
      "23  RT @pitsr4me: Hmm  Are you putting two and tw...  query  \n",
      "24  What are the odds that anyone survives one mas...  query  \n",
      "25  RT @pitsr4me: Hmm  Are you putting two and tw...  query  \n",
      "26  Research. Who inside the bar were also witness...  query  \n",
      "27  Hmm  Are you putting two and two together\\n...  query  \n",
      "28  RT @LauraLoomer: ISIS claimed responsibility f...   deny  \n",
      "29  RT @taxcutsforall: What are the odds?  When is...  query  \n",
      "30  RT @sovereignity77: WHAT! #LasVegasShooting su...  query  \n",
      "31  WHAT! #LasVegasShooting survivors attacked aga...  query  \n",
      "32  \"Las Vegas\" is trending on Twitter, because of...  query  \n",
      "33  @sharonfromNOLA Did you fully read the report...   deny  \n",
      "34  And there it is... Two days after dems take th...  query  \n",
      "35  RT @LauraLoomer: 12 were killed in a Californi...  query  \n",
      "36  Do you remember the #LasVegasShooting? https:/...  query  \n",
      "37  @EdKrassen @Real_ImoGirl Irony: it has been re...   deny  \n",
      "38  RT @LauraLoomer: Yup. Just like the #LasVegasS...   deny  \n",
      "39  I'm so sorry for your loss. \\n\\nIt's ridiculo...   deny  \n",
      "40  RT @Marina_Sirtis: So do we need armed guards ...  query  \n",
      "41  RT @themiamivoice: It's just another mass shoo...   deny  \n",
      "42  RT @AngelaBelcamino: Answer this... \\n\\nIf you...  query  \n",
      "43  RT @dave__whiteside: Thoughts on yet another m...   deny  \n",
      "44  RT @sjredmond: Why has gun violence research h...  query  \n",
      "45  RT @BettyBowers: Guns don't kill people?  Well...  query  \n",
      "46  RT @austinjrandall: two days ago, we had the o...  query  \n",
      "47  RT @CommonSenseWise: Be it #Californiashooting...   deny  \n",
      "48  RT @AynRandPaulRyan: According to Republicans,...   deny  \n",
      "49  RT @Harryslaststand: The last thing America ne...   deny  \n",
      "50  RT @nachi316: He doesnt look black.\\nHe sures...   deny  \n",
      "51  RT @taradublinrocks: Why does @PrisonPlanet st...  query  \n",
      "52  RT @austin_sam001: Oh, so there was another ma...  query  \n",
      "53  RT @kharyp: Mental illnesses didnt shoot &amp...   deny  \n",
      "54  RT @KingAtlasLIVE: Blaming the NRA for gun vio...   deny  \n",
      "55  RT @AndyOstroy: Another shooting by a white ma...   deny  \n",
      "56  RT @taradublinrocks: Why does @PrisonPlanet st...  query  \n",
      "57  @ABC This right here, makes it very clear to m...  query  \n",
      "58  What the hell is happening when our country su...  query  \n",
      "59  another shooting in the states? when does it e...  query  \n"
     ]
    }
   ],
   "source": [
    "newdata = lasvegas.append(california)\n",
    "newdata = newdata.reset_index()\n",
    "print(newdata)\n",
    "\n",
    "newdata.to_csv('newdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_labels_preproc.pickle', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "    \n",
    "labels = newdata['label'].tolist()\n",
    "tweets = newdata['tweet'].tolist()\n",
    "ids = newdata['id'].tolist()\n",
    "\n",
    "# print(ids)\n",
    "\n",
    "# print(labels)\n",
    "# print(len(tweets))\n",
    "\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "#will have to add the following custom \n",
    "custom_stopwords = set([\"http\", \"rt\", \"co\"])\n",
    "all_stopwords = default_stopwords | custom_stopwords\n",
    "# all_stopwords = default_stopwords\n",
    "eng_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    \n",
    "for i in range(len(ids)):\n",
    "    text = tweets[i]\n",
    "    words = RegexpTokenizer('\\w+').tokenize(text)\n",
    "    #remove single character words\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    #removing numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "    #convert to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    #stem the words\n",
    "    words = [eng_stemmer.stem(word) for word in words]\n",
    "    #remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    dataset[(ids[i],labels[i])] = words\n",
    "    \n",
    "with open('training_labels_preproc_new.pickle', 'wb') as handle:\n",
    "    pickle.dump(dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer...\n",
      "  (0, 617)\t1\n",
      "  (0, 1002)\t1\n",
      "  (0, 1703)\t1\n",
      "  (0, 3812)\t1\n",
      "  (0, 8882)\t1\n",
      "  (0, 13124)\t1\n",
      "  (0, 15633)\t1\n",
      "  (0, 16975)\t1\n",
      "  (0, 17819)\t1\n",
      "  (0, 18057)\t1\n",
      "  (0, 18249)\t1\n",
      "  (0, 20691)\t1\n",
      "(1, 25081)\n",
      "Tfidf...\n",
      "  (0, 20691)\t0.30032806953550634\n",
      "  (0, 18249)\t0.28628763530724693\n",
      "  (0, 18057)\t0.28628763530724693\n",
      "  (0, 17819)\t0.28628763530724693\n",
      "  (0, 16975)\t0.28628763530724693\n",
      "  (0, 15633)\t0.30032806953550634\n",
      "  (0, 13124)\t0.28628763530724693\n",
      "  (0, 8882)\t0.28628763530724693\n",
      "  (0, 3812)\t0.28628763530724693\n",
      "  (0, 1703)\t0.28628763530724693\n",
      "  (0, 1002)\t0.28628763530724693\n",
      "  (0, 617)\t0.28628763530724693\n",
      "(1, 25081)\n"
     ]
    }
   ],
   "source": [
    "vectorize_data('training_labels_preproc_new.pickle',3,'trigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4298, 25081) (4298,)\n",
      "(1049, 25081) (1049,)\n",
      "For MultinomialNB, with trigram bow model: \n",
      "Classification accuracy:  0.7397521448999047\n",
      "Confusion matrix:  [[771   7   0   0]\n",
      " [ 89   5   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.42      0.05      0.09        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.59      0.74      0.64      1049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4298, 25081) (4298,)\n",
      "(1049, 25081) (1049,)\n",
      "For MultinomialNB, with trigram tfidf model: \n",
      "Classification accuracy:  0.7426120114394662\n",
      "Confusion matrix:  [[778   0   0   0]\n",
      " [ 93   1   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      1.00      0.85       778\n",
      "    support       1.00      0.01      0.02        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.64      0.74      0.63      1049\n",
      "\n",
      "(4298, 25081) (4298,)\n",
      "(1049, 25081) (1049,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SGDClassifier, with trigram bow model: \n",
      "Classification accuracy:  0.7397521448999047\n",
      "Confusion matrix:  [[774   4   0   0]\n",
      " [ 92   2   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.33      0.02      0.04        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.58      0.74      0.63      1049\n",
      "\n",
      "(4298, 25081) (4298,)\n",
      "(1049, 25081) (1049,)\n",
      "For SGDClassifier, with trigram tfidf model: \n",
      "Classification accuracy:  0.7416587225929456\n",
      "Confusion matrix:  [[774   4   0   0]\n",
      " [ 90   4   0   0]\n",
      " [ 71   0   0   0]\n",
      " [106   0   0   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    comment       0.74      0.99      0.85       778\n",
      "    support       0.50      0.04      0.08        94\n",
      "       deny       0.00      0.00      0.00        71\n",
      "      query       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.60      0.74      0.64      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb3 = MultinomialNB()\n",
    "run_classification('bow_trigram_new.pkl', 'training_labels_bow_trigram_new.pickle', 'For MultinomialNB, with trigram bow model: ', nb3)\n",
    "\n",
    "nb6 = MultinomialNB()\n",
    "run_classification('tfidf_trigram_new.pkl', 'training_labels_tfidf_trigram_new.pickle', 'For MultinomialNB, with trigram tfidf model: ', nb6)\n",
    "\n",
    "svm3 = linear_model.SGDClassifier()\n",
    "run_classification('bow_trigram_new.pkl', 'training_labels_bow_trigram_new.pickle', 'For SGDClassifier, with trigram bow model: ', svm3)\n",
    "\n",
    "svm6 = linear_model.SGDClassifier()\n",
    "run_classification('tfidf_trigram_new.pkl', 'training_labels_tfidf_trigram_new.pickle', 'For SGDClassifier, with trigram tfidf model: ', svm6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
